# Smart India Hackathon Workshop
# Date: 25.03.2025
## Register Number: 212224220059
## Name: MANASA S
## Problem Title
SIH 1653: Web based Selector-Applicant Simulation Software
## Problem Description
Background: Recruitment and Assessment Centre (RAC) under DRDO, Ministry of Defence carries out interviews for applications received against advertised vacancies and for promotion to next higher grade for scientific manpower inducted within DRDO. Description: The process of interviewing is a challenging task. An unbiased objective interviewing process helps identify the right talent. The basic process of an interview involves posing a set of questions by an interviewer and thereafter evaluating responses from candidates. Thus, the questions asked should be relevant and match the area/ expertise of the applicant and the responses should also be of relevance w.r.t. the question asked. Expected Solution: The proposed solution should provide experts as well as candidates a real life Board Room experience, starting with initial ice-breaking questions leading to in-depth techno-managerial (depending on the level of candidate) questions. It shall also be able to provide a quantifiable score for experts as well as the candidate for the relevancy of questions w.r.t. the area/ expertise of the applicant. Similarly, candidate responses should also be graded for relevancy w.r.t. the question asked, finally assisting in arriving at an overall score for the subject knowledge of the candidate and thus his/ her suitability against the advertised post.

## Problem Creater's Organization
Ministry of Defence

## Idea
Based on the applicant's resume and the job requirements, the software will recommend or generate tailored questions.

These questions should be categorized based on the applicant's technical expertise (e.g., software engineering, electrical engineering, material science, etc.) and the level of the job (entry-level, mid-level, senior-level).

The questions will start with ice-breaking queries to help the applicant feel comfortable and gradually move into more complex, domain-specific technical questions.


## Proposed Solution / Architecture Diagram
Candidates’ responses to each question should be graded for relevancy and depth. The system can use predefined evaluation metrics based on the relevancy of the answer to the question.

The evaluator (interviewer) should be able to give a quantitative score for each response, based on a set of predefined criteria, ensuring consistency in evaluation.

## Use Cases
Both candidates and interviewers should receive a score at the end of the interview.

Candidate scores will be based on the relevancy of their answers to the questions asked, the depth of knowledge demonstrated, and the clarity of communication.

The interviewer’s score will be based on the quality and relevance of the questions asked, as well as their ability to assess the candidate’s responses accurately.

## Technology Stack
The system should provide real-time feedback to both the interviewer and candidate, highlighting areas of strength and improvement for the candidate.

Interviewers can also rate their own performance and question quality to help identify areas where they might need additional training or focus.

## Dependencies
The software will also allow new interviewers to practice. A simulation mode will let interviewers interact with a "mock" candidate (or AI-generated responses) to refine their questioning techniques, response evaluation, and scoring abilities.
